# Design Proposal for NVMe Out-Of-Band Management

Author:
  Hao Jiang (Haoj)

Other contributors:
  'None'

Created:
  Feb 10, 2022

## Problem Description

Typically the NVMe storage devices are directly accessed and controlled by the software on Host machines. However, when the control domain are shifting from CPU to BMC, the NVMe management is transiting into Out-Of-Band instead of inband, meaning the DC software will directly talk through BMC for all the NVMe Management tasks, while letting the computation node doing pure I/O transaction. The classic scenario of NVMe management tasks will include but not be limited to:

* enumerating and identifying the hierarchy of each NVMe device in order to build a device tree by DC software, which will include the subsystem, controllers, namespaces, etc.
* monitoring status for NVMe components for diagnostics. Examples of the status matrix will be:
  * Health status for the NVMe subsystem including warning(SW), temperature(CTMP), drive life(PDLU), contoller status(CCS).
  * Configuration for NVMe controller such as SMBus/I2C frequency and MCTP Transmission Unit Size.
  * Features for controllers involving Arbitration, Power Management, Controller Metadata, etc.
  * Identify information for controller and namespace, such as FW info, SN/MN, IEEE OUI Identifier(IEEE), Maximum Data Transfer Size(MDTS), RTD3 Resume Latency(RTD3R), RTD3 Entry Latency(RTD3E), etc
* collecting the error logs generated by NVMe controllers for repair.
* performing secured operations on encrypted NVMe device via [TCG storage spec](https://trustedcomputinggroup.org/wp-content/uploads/TCG_Storage_Architecture_Core_Spec_v2.01_r1.00.pdf), such as lock/unlock the device/LBA band, read SED DataStore, etc
* config the NVMe features (Command Arbitration, Power State, etc) before logging-in and cleaning up the namespaces and/or entire subsystem after log-out
  
Therefore, An NVMe software stack must be built inside BMC to represent the NVMe specific aspects and capabilities beyond a generic storage device.

## Background and References

NVM Express Workgroup defines a command set and architecture for managing NVMe
storage in the specification of NVMe Management Interface([NVMe-MI](https://nvmexpress.org/developers/nvme-mi-specification/)), via which the Data Center could achieve the out-of-band(OOB) control to the NVMe devices through BMC.

On the north boundary, [Swordfish](https://www.snia.org/forums/smi/swordfish),
which is an extension of DMTF Redfish specification, providing a RESTful API for NVMe storage device as well as the mapping guidance between the standard NVMe definition and RESTful schema.

Other useful repositories that are already merged into OpenBMC are:

* [libmctpd](https://github.com/openbmc/libmctp): An implementation for
Management Component Transport Protocol(MCTP).
* [mtcpd](https://github.com/CodeConstruct/mctp): Userspace tools for MCTP stack
management.
* [libnvme-mi](https://github.com/CodeConstruct/libnvme): library to implement
NVMe-MI functions over MCTP message.

They serve as the underlay driver to transmit NVMe messages between BMC and the target device.

Currently there are couple of OpenBMC services also striking the NVMe area. I list them below:

* [NVMe Sensor](https://github.com/openbmc/dbus-sensors/commit/b669b6b54b1bfa499380e518bf508b672dd9a125)
* [phosphor-nvme](https://github.com/openbmc/phosphor-nvme)

NVMe Sensor works as a standard phosphor thermal sensor, which is dynamically enumerated as a reactor of Entity-Manager. NVMe sensor can be integrated into pid control or exposed as regular sensor on Redfish or IPMI.

phosphor-nvme can serve the similar purpose as a thermal sensor, nevertheless, it enumerates statically from its only configration file and it exposes the inventory information. Besides, phosphor-nvme also expose the basic SMART log which is defined in the Basic Management protocol.

## Requirements

The scope of this proposal should be the same as what can be achieved by the inband NVMe management via PCIE link. The DC software should already build an infrastructure to operate the storage devices through libnvme(or similar library) on HOST CPU. The DC infrastructure will need to migrate the tasks to RESTful API of Redfish/Swordfish on BMC.

The NVMe device should follow the NVMe specification to implement an OOB NVMe controller on I2C/SMBus. The controller should perform as a NVMe typed MCTP endpoint to process the NVMe over MCTP messages. BMC has already possessed the ability to send/receive NVMe MCTP packages to the device controller thanks to the existing driver library on OpenBMC(i.e libmctp/mctpd/libnvme-mi).

The proposed NVMe daemon will fill in the gap between the Redfish server(BMCWeb) and the NVMe driver. It will provide standard NVMe subsystem topology and APIs to the DBus WITHOUT:

1. owning self-defined driver implementation.
2. reinviting any new APIs outside NVMe specification.
3. caching the data.

## Proposed Design

### NVMe Subsystem Enumeration

The implementation of the proposal will create a daemon called NVMed, which works as a reactor to the openBMC's dynamical enumeration stack(A.K.A Entity-Manager). The daemon will monitor the NVMe DBus
interface of `xyz.openbmc_project.Configuration.NVMe`, which stands for a NVMe OOB controller of type of MCTP over I2C. The NVMe device can usually be probed from the VPD info within EEPROM or similar I2C protocol device, which should be convered by E-M instead of NVMe scope.

```json
{
  "Exposes": [
    {
      "Type": "NVMe",
      "Name": "ExampleName",
      "MCTPBus": "I2C",
      "Address": "0x1d"
     }
  ],
  "Probe": "xyz.openbmc_project.FruDevice({'BOARD_PRODUCT_NAME': 'ExampleProduct'})"
}
```

Once a NVMe configuration is enumerated on the DBus, the NVMed will initiate the MCTP remote ep for the NVMe subsystem via [MCTPd DBus interface](https://github.com/CodeConstruct/mctp#mctpd-usage) and post the nvme subsystem object on its DBus:

```text
/xyz/openbmc_project/nvme/(subsys)1
```

The nvme subsys object will expose two interfaces on DBus as well:

* xyz.openbmc_project.NVME.MCTPInfo: an association to MCTPd ep object.
* xyz.openbmc_project.NVME.MI: a collection of NVMe Management Interface Commands to the target subsystem.

The following table describe the NVMe subsystem interfaces and their properties/methods:
| Interface                         | Property/Method    | Comment                                      |
| --------------------------------- | ------------------ | -------------------------------------------- |
| xyz.openbmc_project.NVME.MCTPInfo | Path               | DBus object path of nvme endpoint from mctpd |
| xyz.openbmc_project.NVME.MI       | HealthStatusPoll() | NVMe MI Spec 5.6                             |
|                                   | ReadData()         | NVMe MI Spec 5.7                             |

Upon the NVMe device removal, the NVMed will also delete the DBus objects and close the MCTP ep.

### NVMe Controller Enumeration

The NVMed will also enumerate the controllers within the subsystem:

```text
/xyz/openbmc_project/nvme/(subsys)1/(controller)0
                                   /(controller)1
                                   /(controller)2
```

In SR-IOV(Figure 19, [NVMe Base Spec rev2.0a](https://nvmexpress.org/wp-content/uploads/NVMe-NVM-Express-2.0a-2021.07.26-Ratified.pdf)),
VFs are shown as sub-objects of its root PF:

```text
/xyz/openbmc_project/nvme/(subsys)1/(PF)0
                                   /(PF)0/(VF)1
                                         /(VF)2
```

An interface of NVMe Admin Command set are attached to each controller. An example of the interface is shown as:

| Interface                        | Property/Method | Comment             |
| -------------------------------- | --------------- | ------------------- |
| xyz.openbmc_project.NVME.MIAdmin | GetLogPage()    | NVMe Base Spec 5.16 |
|                                  | Identify()      | NVMe Base Spec 5.17 |
|                                  | SecuritySend()  | NVMe Base Spec 5.26 |
|                                  | SecurityRecv()  | NVMe Base Spec 5.25 |

## Alternatives Considered

### phosphor-nvme

It is not feasible to reuse phosphor-nvme, given its static enumeration stack. The SSD tray should be plugable and swappable for repairablity and other maintance jobs.

phosphor-nvme also includes some non-NVMe peripherals, such as LED and GPIO, which are not mandatory and standardizable in NVMe spec. In addition, the underlay protocol of phosphor-nvme is "NVM Express Basic Management Command"[^1], which is i2c block read instead of NVMe message over MCTP.

### nvme sensor

The NVMe sensor can be upgraded for the proposed NVMe OOB tasks if only:

* Parse and mapping all required NVMe properties into the existing DBus interface(`xyz.openbmc_project.inventory.item.storage`, `xyz.openbmc_project.inventory.item.drive`, etc)
* local polling the attributes within the DBus daemon.

The proposed design is against this idea for the reason:

* Many of the properties are NVMe specific and not common to other storage device such as eMMC or HDD. Some of the data field are Vendor Unique whose parser doesn't exist for BMC.
* Asynchronous Events are prohibited[^2] on OOB mechanism and OOB commands may interfere with Inband transaction[^3]. As the result, only the DC software should coordinate the OOB polling with other tasks interacting with the NVMe device. One example is that the clean-up service will poll the identification of the controller/namespace and retrieve error logs when the user logging out.  

The sensor is good to represent one aspect of the subsystem for a certain autonomous task, e.g. a thermal sensor of pid control. Given the complexity, variance, and extensibility of the NVMe management, we should avoid re-inventing the NVMe spec on DBus as a sensor.

As for enumerating the NVMe subsystem, NVMed can work as a reactor of MCTPd other than E-M. That is to say, instead of `xyz.openbmc_project.Configuration.NVMe`, E-M will create `xyz.openbmc_project.Configuration.MCTP`, on which the MCTPd will probe. And NVMed will monitor the `xyz.openbmc_project.MCTP.Endpoint` with `SupportedMessageTypes` has 04h(NVMe Type).

## Impacts

Given NVMed is exposing the standard NVMe interface, others should be easy to
build their own NVMe service on top of NVMed. For example a NVMe sensor daemon
can construct a standard thermal sensor by pulling SMART log. Another example
will be a NVMe log daemon can expose the log page from NVMe device and tunneling
the logs to Redfish/IPMI/journald. However, since these functionality is outside
the definition of NVMe subsystem, it is not within the scope of NVMed.

The physical transportation is also out of the scope of NVMed, which is purely
depending on the MCTP implementation. So it is going to be fairly easy to
support MCTP over PCI VDM if the underlay supports it.

### Organizational

The NVMed can live in parallel within phosphor-nvme, given the functionality of
these two daemon are complementary.

## Testing

* Unit test can be done with MOCK libnvme-mi.
* Regression test can be performed with supported SSD.

[^1]: In "Appendix A Technical Note of NVMe-MI spec", it says "This appendix describes the NVMe Basic Management Command and is included here for informational purposes only. The NVMe Basic Management Command is not formally a part of this specification and its features are not tested by the NVMe Compliance program."

[^2]: Figure 114, NVM Express Management Interface Specification, Rev 1.2

[^3]: "NVMe Admin Commands over the out-of-band mechanism may interfere with host software", Page 107, NVM Express Management Interface Specification, Rev 1.2
