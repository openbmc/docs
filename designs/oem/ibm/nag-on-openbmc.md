# NAG design

Author: Dhruvaraj Subhashchandran, dhruvibm

Other contributors: Aravind T Nair, aravynd

Created: 02 Feb 2023

## Problem Description

On the IBM OpenPOWER systems, event logs and hardware isolation records are
created after encountering a hardware failure. These records will be resolved
once the repair action is performed. However, in certain situations, the service
engineer or customer can defer the service action for some of the failures. In
such cases, there are chances to miss such failure records, and the system may
fail unexpectedly, causing undesirable impacts. This problem is overcome by
sending periodic error events to remind about the failed parts present in the
system.

## Background and References

There are two parts to the NAG functionality. One is a periodic logging of the
events if any failed parts are present in the system, and the other one is
generating the data about failed parts to include in the reports requested by
external clients.

### Glossary

- **Hardware Isolation**: Removing the failed hardware unit from service if
  redundant resources are available
  [more info](https://github.com/openbmc/docs/blob/master/designs/guard-on-bmc.md)
- **FCO**: Field Core Override is a method of enabling only a limited number of
  processor cores in the system
- **NAG**: Process of reminding customers or service engineers about the
  presence of failed parts in the system.
- **SRC**: System Reference Code (SRC) in IBM Power Systems refers to a
  firmware-level code used to diagnose and handle hardware-related issues within
  the system. SRC provides a standardized set of codes for hardware components
  and sub-systems to use when reporting problems to the operating system. The
  SRC code provides a clear and concise way to identify the source of a hardware
  error, allowing administrators to quickly and accurately diagnose and resolve
  any issues that may arise.
- **Callout**: A hardware callout in IBM Power Systems refers to a notification
  from the system hardware to the operating system or firmware, indicating a
  failure or malfunction of a component or sub-system. The callout is a way for
  the hardware to communicate the issue to the software, which can then take
  appropriate action, such as logging the error, reporting it to the user, or
  attempting to recover from the failure.
- **Repair and Verify:** Repair and Verify is a set of directed maintenance
  procedures that can be followed to resolve a system problem. R&V presents a
  possible solution, such as removing and replacing certain system components
  and displays a series of panels that provide specific directions for doing so.

## Requirements

Create an error log if there are unresolved guard records or unresolved error
logs in the following scenarios

- Power on
- After repair and verify
- Periodically once a month
- After a BMC failover or a BMC reboot from runtime

Create failure data when requested - A JSON file will be generated with the
following information whenever a request is received from an external
application creating call home data.

- Include all hardware isolation records
- All deconfiguration due to a special condition like FCO
- All unresolved PELs with a special hardware error bit set

## Proposed Design

### Creating NAG event

A command line application will be developed to check whether there are error
logs or isolation records matching the nag criteria. This application will
create a special error event if a matching criterion is found. This application
will be invoked with a systemd service file linked to the systemd timer service
and systemd targets for host power on, BMC reboot, and BMC failover. Creating a
nag event during repair and verify should be handled by the application handling
that operation.

### Generating NAG Data

The nag data will be added as part of a new type of user-requested BMC dump. The
data will be generated by a new command line application, which will query the
necessary services, fill in the data in JSON format and write to a file. This
command line application will be invoked by dreport while creating the dump.
This will enable users to request this type of dump whenever nag data is needed.

#### Contents of hardware failure data

Serviceable event data: All unresolved serviceable events with hardware
isolation set will be retrieved and added to the JSON data. The following fields
will be added.

- PLID: A unique id of the hardware failure event
- SRC: An indicator of the type of error
- Date/Time: Date and Time of the event
- Callout Priority: Priority of the hardware callout
- Location code: Location code of the FRU in the system
- CCIN: CCIN of the FRU
- Serial Number: Serial Number of the failed FRU

Certain types of hardware units can be isolated when an error is encountered.
For such units, the following additional data will be added

- Type: Type of the isolated unit
- Location Code: The location code of the FRU containing the unit
- State: Current state of the unit
- Reson: Type of hardware isolation
- Permanent: Indicates whether the unit is isolated permanently

Hardware isolation data: There are some units that get isolated due to some
special conditions of the system or by user request. Such isolations may not
have associated error events. The following data will be added for such records.

- Type: Type of the isolated unit
- Physical path: Physical path of the isolated unit
- Location code: The location code of the FRU containing the unit
- State: Current state of the unit, whether isolated from service of active
- Reason: The reason for isolating hardware

##### Json file structure

System details, with system type and model

    {
       "SYSTEM":{
          "SYSTEM_TYPE":"SYSTEM_TYPE_MODEL",

All serviceable events with permanent hardware isolation or hardware error

      "SERVICABLE_EVENT":{
         "CEC_ERROR_LOG":[
            {
               "ERR_PLID":"Error Log Id",
               "SRC":"SRC",
               "DATE_TIME":"Timestamp",
               "CALLOUT":{
                  "PRIORITY":"Callout priority",
                  "FRU_SYMBOLIC_FRU":"Location code",
                  "CCIN":"CCIN of the FRU",
                  "SERIAL_NUMBER":"Serial Number of the FRU",
                  "PART_NUMBER":"Part Number of the FRU"
               },
               "RESOURCE_ACTIONS":{
                  "TYPE":"Type of the guarded unit",
                  "LOCATION_CODE":"Location code of the FRU",
                  "CURRENT_STATE":"Current state of the unit",
                  "REASON_DESCRIPTION":"Guard Type",
                  "GARD_RECORD":"True if the guard record is created"
               }
            },

All hardware isolation due to special reasons like FCO or user requested
hardware isolation

      "DECONFIGURED":[
         {
            "TYPE":"Type of the deconfigured using",
            "PHYS_PATH":"Physical Path",
            "LOCATION_CODE":"Location code",
            "CURRENT_STATE":"DECONFIGURED",
            "PLID":"Error log id",
            "REASON_DESCRIPTION":"Deconfiguration Reason"
         }
      ],

Hardware isolation policies and FCO value

          "POLICY":{
             "FCO_VALUE":"FCO Number",
             "MASTER":"Master guard enabled or not",
             "PREDICTIVE":"Predictive guard enabled or not"
          }
       }
    }

## Alternatives Considered

- Create a new redfish interface to retrieve nag data
- Add the nag data as part of the existing BMC dump
- Add nag data to the error log
- Creating the hardware failure data in XML

## Impacts

OpenBMC impacts

- **Hardware Isolation manager**: Need to add a new command line to generate nag
  data
- **Dump Manager**: Add an additional dump type option while creating BMC dump,
  update BMC dump packaging to call the command-line application to collect nag
  data, Add a feature flag to enable this feature only for selected systems,

Redfish clients

- Should implement methods to open the dump and retrieve the data.

## Testing

TBD
