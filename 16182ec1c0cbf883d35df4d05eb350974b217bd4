{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "6ee20e91_dda8a77a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000388
      },
      "writtenOn": "2022-03-18T04:26:15Z",
      "side": 1,
      "message": "The table is very hard to read and the bullets are only rendered on markdown + html viewer (visual studio or atom supports it) so please review it from a markdown viewer!",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8a457782_869d7afe",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-03-18T05:16:19Z",
      "side": 1,
      "message": "Have you seen the existing solutions for this? Things like hiomap and the astlpc MCTP channel, which use a shared-memory area plus some synchronisation. I feel like most of the complexities have already been handled by those.\n\n(those currently use LPC to share the memory region, but there\u0027s no reason that can\u0027t be extended to PCIe instead...)",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a9a288ab_8c24cbb8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-03-18T05:18:45Z",
      "side": 1,
      "message": "Have you thought about using this: https://github.com/openbmc/libmctp/blob/master/docs/bindings/vendor-ibm-astlpc.md ? This would give you a compliant DMTF stack for doing RDE as well.\n\nIt can be extended to use a ring buffer to queue packets.",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b642619f_2eb11ac6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000388
      },
      "writtenOn": "2022-03-18T17:09:05Z",
      "side": 1,
      "message": "Hi Jeremy, I didn\u0027t look through MCTP channel too thoroughly and it looks like Andrew also commented on it - looking into it, it does seem like it currently uses LPC shared memory region, which for Nuvoton is 4KB and is too small for our usecase which is why it was discounted.\n\nBut yes, if we were to extend this to use PCI Mailbox, perhaps this is something we could consider.. let me look into how that would look like, thanks for the suggestion.",
      "parentUuid": "8a457782_869d7afe",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ce7690cf_abb6d847",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000388
      },
      "writtenOn": "2022-03-18T17:09:05Z",
      "side": 1,
      "message": "Hi Andrew, it looks like Jeremy and you had the same thoughts. As mentioned in Jeremy\u0027s comment, we discounted LPC shared memory methods due to Nuvotn\u0027s LPC shared memory buffer being only 4KB (for NPCM 7xx generation at least) which was not big enough for our usecase.\n\nI\u0027ll look into whether we can extend it to use PCI Mailbox instead, similar to how phosphor-ipmi-flash currently supports both LPC and PCI Mailbox / P2A.",
      "parentUuid": "a9a288ab_8c24cbb8",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4d50854f_4c55cad5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000188
      },
      "writtenOn": "2022-03-18T17:30:11Z",
      "side": 1,
      "message": "Jeremy, doesn\u0027t this require an ACK from the BMC to set up the buffer initially? I assume the goal isn\u0027t to hog the buffer from the host side in perpetuity in the event that we need the asynchronous fast access.\n\nOur requirement for this is a memory window the host can write to with no waiting on software BMC side. If we have to ask MCTP for some memory right before using it, then it does not meet this requirement.",
      "parentUuid": "b642619f_2eb11ac6",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "17c50499_4bf517a1",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000388
      },
      "writtenOn": "2022-03-18T17:36:09Z",
      "side": 1,
      "message": "Initial set up requiring ack is fine (as our circular buffer header requires BMC and host side acks), however yes, if an ack is required for each memory access then that doesn\u0027t meet our requirement.\n\nI realize I didn\u0027t note that explicitly in the requirement of this doc (I noted it in some other parts of the doc), I\u0027ll add that in.",
      "parentUuid": "4d50854f_4c55cad5",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a1fe4662_acd08673",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000388
      },
      "writtenOn": "2022-03-18T22:18:20Z",
      "side": 1,
      "message": "It looks like from https://github.com/openbmc/libmctp/blob/master/docs/bindings/vendor-ibm-astlpc.md#host-packet-transmission-sequence the TX, RX ownership transfers mean that the host won\u0027t be able to \"fire and forget\" unfortunately so I do not believe this is suitable for our usecase.",
      "parentUuid": "17c50499_4bf517a1",
      "revId": "16182ec1c0cbf883d35df4d05eb350974b217bd4",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    }
  ]
}