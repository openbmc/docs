{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "3f5df87b_ccc5edd5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "Hi Hao Jiang, thanks for the proposal. It sounds like we\u0027re missing support for a few NVMe use-cases in OpenBMC and that you\u0027d like to resolve that. Great!\n\nI\u0027ve added some comments around how I think we can work this document to help describe what it is you need. I think that can only be achieved by addressing some structural issues with the text, so -1 for now.",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d3097a12_9683a95f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-06-02T20:33:36Z",
      "side": 1,
      "message": "I am back from vacation. Thanks for the suggestion. I will update the doc soon. ",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0e5ed1ca_f4ad6967",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "+1 for Andrew and Ed\u0027s comments - this needs a bit of a rework to start with the problem statement before the solution is defined. Once that\u0027s done, we can review with a focus on the proposed design rather than the document structure.\n\nThe major design point will be the relation with the existing implementations; if this can\u0027t be implemented as part of one of the existing codebases, then how it would coexist. I suspect that your proposed nvmed exposes a richer set of potentially NVMe-specific management functionality, but we\u0027d need to make that explicit, and describe how that works within the existing systems.\n\nI\u0027ve added a couple of responses in line with the existing mctp/etc infrastructure too.",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0af69f8b_1a778ba3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 1,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "Coming back to this now that I\u0027ve read through, I think we need to change the title to something representing the problems the proposal is solving, not the name of the proposed solution.",
      "range": {
        "startLine": 1,
        "startChar": 2,
        "endLine": 1,
        "endChar": 57
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2554d05e_5bfb311f",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 1,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "0af69f8b_1a778ba3",
      "range": {
        "startLine": 1,
        "startChar": 2,
        "endLine": 1,
        "endChar": 57
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e2d2d6c8_bf410508",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 11,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "nit, whitespace error.",
      "range": {
        "startLine": 11,
        "startChar": 0,
        "endLine": 11,
        "endChar": 2
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ea30d5a7_d84dfab3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 11,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "e2d2d6c8_bf410508",
      "range": {
        "startLine": 11,
        "startChar": 0,
        "endLine": 11,
        "endChar": 2
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d91db807_c1f7a182",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 17,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "I think this section needs a few changes:\n\n1. It shouldn\u0027t mention a proposed solution (nvmed), it\u0027s meant to be a problem description\n2. The problems should be discussed in terms of use cases. What function elsewhere in an OpenBMC system would be requesting the GetLogPage or Identity functions? Why?\n3. The description should cover deficiencies in existing support for NVMe drives in OpenBMC, specifically phosphor-nvme and nvmesensor from dbus-sensors\n\nAs a part of dealing with 3, if there\u0027s a reason either of those components *shouldn\u0027t* contain the support you need, then that also needs to be covered, and argued in the \"Alternatives Considered\" section (the fact the entire content of \"Alternatives Considered\" is \"Stated in the above chapter\" is a bit of a red flag for me in terms of how you\u0027ve structured your argument here).\n\nIMO the first time we should hear about \"nvmed\" is in the \"Proposed Design\" section.",
      "range": {
        "startLine": 14,
        "startChar": 0,
        "endLine": 17,
        "endChar": 55
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ce4e7117_9a32b3de",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 17,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "+1 to Arjs comments;  We need to call out the problem being solved here.  The existing solutions ( phosphor-nvme and nvme-sensor) have a way to enumerate nvme resources and put them on dbus, admittedly, in a more simplistic way than we\u0027d like to see, but code can always be added to one of those.",
      "parentUuid": "d91db807_c1f7a182",
      "range": {
        "startLine": 14,
        "startChar": 0,
        "endLine": 17,
        "endChar": 55
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "bf296475_86d48caf",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 27,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "This is only background if you already consider the design accepted, which isn\u0027t the purpose of this document. The purpose of this document is to help you help the community to converge on the architecture of a solution to your problems, where your problems are your use-cases that OpenBMC doesn\u0027t currently support.\n\nThings I\u0027d consider background information are the requirements and environment that lead to the existence of both phosphor-nvme and nvmesensor from dbus-sensors.",
      "range": {
        "startLine": 26,
        "startChar": 0,
        "endLine": 27,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d9f88dc1_24513c8b",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 27,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Agree. The design is kind of under discussion internally for a while, making it more like a refined-decision when I wrote the proposal. \n\nAlso rephased the section, stripping away the design for nvmed and adding more background topic on phosphor-nvme and nvme sensor.",
      "parentUuid": "bf296475_86d48caf",
      "range": {
        "startLine": 26,
        "startChar": 0,
        "endLine": 27,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "88214563_fb112a34",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 47,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "IMO you need to flip the orientation here to describe these as deficiencies in phopshor-nvme without mentioning nvmed.\n\nYou also need to argue why it\u0027s not possible to change the design direction of phosphor-nvme and motivate the need for a third code-base to handle NVMe drives.",
      "range": {
        "startLine": 40,
        "startChar": 0,
        "endLine": 47,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "727d563d_c703d990",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 47,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "\u003e IMO you need to flip the orientation here to describe these as deficiencies in phopshor-nvme without mentioning nvmed.\n\u003e \n\u003e You also need to argue why it\u0027s not possible to change the design direction of phosphor-nvme and motivate the need for a third code-base to handle NVMe drives.\n\n+1 this second sentence.  We already have two nvme-speaking daemons;  phosphor-nvme for static stack, and nvmesensor for entity-manager reactor stacks, from these requirements, it isn\u0027t clear why we\u0027d need another.\n\nFWIW, I have no problem if the code for nvme-sensor was completely reworked to support these goals, and it\u0027s already a reasonably well tested starting point for this stuff.",
      "parentUuid": "88214563_fb112a34",
      "range": {
        "startLine": 40,
        "startChar": 0,
        "endLine": 47,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2bc27091_725c8ee5",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 47,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "The argument against the existing nvme project is stated in the alternative section.",
      "parentUuid": "727d563d_c703d990",
      "range": {
        "startLine": 40,
        "startChar": 0,
        "endLine": 47,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "120de91a_ff0875a6",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 51,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "Requirements are behaviours derived from use-cases, not a concrete solution. A concrete solution should follow from the requirements. Again this makes it feel like a write-down-the-decisions document rather than a drive-the-decision-making-process document, but this is the first time I\u0027m hearing of the decision making process.",
      "range": {
        "startLine": 51,
        "startChar": 0,
        "endLine": 51,
        "endChar": 5
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "02100d36_37477c4f",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 54,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "What does this mean?  Can you be more specific about what \"initialize\" actually does in this context?  Aren\u0027t nvme controllers initialized by default?",
      "range": {
        "startLine": 54,
        "startChar": 0,
        "endLine": 54,
        "endChar": 61
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "6332bd7a_f1734898",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 54,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "It means setting up the mctp ep with the discovery protocol. It is part of the enumerating the nvme devices. \n\nIt is rather a implement detail rather than the requirement. So I moved it and detailed in the proposed design section.",
      "parentUuid": "02100d36_37477c4f",
      "range": {
        "startLine": 54,
        "startChar": 0,
        "endLine": 54,
        "endChar": 61
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b2fafce8_8b9fe494",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 56,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "Please be more specific here.  What does \"Basic information\" mean?  Thermals?  Inventory?  Ideally this would be derived from your background section about missing features.",
      "range": {
        "startLine": 56,
        "startChar": 0,
        "endLine": 56,
        "endChar": 77
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1af5967b_a8248d77",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 56,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "+1; this could be more descriptive of the MI command set you\u0027re looking to expose.",
      "parentUuid": "b2fafce8_8b9fe494",
      "range": {
        "startLine": 56,
        "startChar": 0,
        "endLine": 56,
        "endChar": 77
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ca6ba8c3_94ab11e4",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 56,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Sure. I listed several examples of the status we need. \n\nAnd this section is moved to the Problem Description.",
      "parentUuid": "1af5967b_a8248d77",
      "range": {
        "startLine": 56,
        "startChar": 0,
        "endLine": 56,
        "endChar": 77
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d7631a13_258437ca",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 57,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "This also should be more specific.  NVMe-MI commands are called \"admin\" commands in the spec.  Presumably you have a few starter use cases?",
      "range": {
        "startLine": 57,
        "startChar": 0,
        "endLine": 57,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ed73bd9b_aaad1340",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 57,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "It looks like this is referring to two separate MI commands sets in the NVMe-MI spec: MI commands, and Admin Commands.",
      "parentUuid": "d7631a13_258437ca",
      "range": {
        "startLine": 57,
        "startChar": 0,
        "endLine": 57,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "68de5e66_48d1b78b",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 57,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Yes, Jeremy is right. \n\nThere are bunch of NVMe matrix we need to read from MI commands and Admin commands. \n\nI listed several in the Problem Description.",
      "parentUuid": "ed73bd9b_aaad1340",
      "range": {
        "startLine": 57,
        "startChar": 0,
        "endLine": 57,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e7115a28_279aed79",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 60,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "\"consumers\" maybe?",
      "range": {
        "startLine": 60,
        "startChar": 55,
        "endLine": 60,
        "endChar": 64
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "92f0d2f4_172805fe",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 60,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "e7115a28_279aed79",
      "range": {
        "startLine": 60,
        "startChar": 55,
        "endLine": 60,
        "endChar": 64
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0d1fb8e0_4cdcd8f8",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 64,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "This is the kind of thing that needs to go into your problem description, phrased in terms of the fact that OpenBMC doesn\u0027t yet support these things.",
      "range": {
        "startLine": 59,
        "startChar": 0,
        "endLine": 64,
        "endChar": 42
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6ff22bc1_f58d282f",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 64,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "+1",
      "parentUuid": "0d1fb8e0_4cdcd8f8",
      "range": {
        "startLine": 59,
        "startChar": 0,
        "endLine": 64,
        "endChar": 42
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d70487cd_e7157e40",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 64,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Yeah. You are right. This is the new features we are requiring. \n\nMoved to problem section",
      "parentUuid": "6ff22bc1_f58d282f",
      "range": {
        "startLine": 59,
        "startChar": 0,
        "endLine": 64,
        "endChar": 42
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "fa87ca57_da22d2a2",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "This interface already exists, and is called NVME1000.  I suspect we don\u0027t need to create a new one, but lets cover that once we\u0027ve figured out the design here, and how it differs from the existing nvme reactors.",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1d07abc2_60415099",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "The NVME1000 is designed for a NVMe thermal sensor who has properties like thresholds. \n\nThe design aims at a totally different type of device and user case. Yeah, let\u0027s try to make consent on the daemon first then the new interface may make its sense.",
      "parentUuid": "fa87ca57_da22d2a2",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "25c5f35e_2ed907a3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T04:45:15Z",
      "side": 1,
      "message": "\u003e The NVME1000 is designed for a NVMe thermal sensor who has properties like thresholds. \n\nThermal sensors over nvme-mi is just the first thing it implemented.  \"NVME1000\" is meant to be the interface for \"supports nvme-mi\" and already has quite a different \n\n\n\u003e \n\u003e The design aims at a totally different type of device and user case.\n\nYou\u0027re using nvme-mi, over mctp to connect to an SSD and run nvme-mi commands.  It\u0027s the same device, same transport, and unless you don\u0027t plan to ever implement thermal sensor support, the same use case.\n\n\u003e Yeah, let\u0027s try to make consent on the daemon first then the new interface may make its sense.\n\nEven if it\u0027s a new daemon, in entity-manager, it would still be an nvme-mi interface, so it would still use nvme1000, but sure, we can take that up in the above.",
      "parentUuid": "1d07abc2_60415099",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "72447884_86620496",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-13T21:13:00Z",
      "side": 1,
      "message": "It sounds like we can share the NVME1000, if multiple daemons (in case we all agree to have a new one) can share the same configuration. \n\nWe also need to take NVMe over MCTP over PCIe VDM into consideration. It should share the same configuration.",
      "parentUuid": "25c5f35e_2ed907a3",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "38a43ae6_0997ed8a",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T23:00:35Z",
      "side": 1,
      "message": "\u003e It sounds like we can share the NVME1000, if multiple daemons (in case we all agree to have a new one) can share the same configuration. \n\nI still don\u0027t think we need multiple daemons here, but sure, in terms of this comment, lets get it changed to NVMe1000 and we can discuss other places.\n\n\u003e \n\u003e We also need to take NVMe over MCTP over PCIe VDM into consideration. It should share the same configuration.\n\nLets discuss that when we get there.  Whether or not it\u0027s the same config name would depend on the requirements of the features.",
      "parentUuid": "72447884_86620496",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8a3c02bc_e7092f7f",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-15T00:38:10Z",
      "side": 1,
      "message": "Agree. Will change to NVMe1000",
      "parentUuid": "38a43ae6_0997ed8a",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f106de94_6d48e2d1",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 71,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-21T02:54:45Z",
      "side": 1,
      "message": "FWIW, if you want to change it to be NVMe in both places, I think it would make a lot of sense and the implementations could be just support both keys.  NVME1000 is a historical thing, but the 1000 isn\u0027t really important in the descriptor.",
      "parentUuid": "8a3c02bc_e7092f7f",
      "range": {
        "startLine": 71,
        "startChar": 14,
        "endLine": 71,
        "endChar": 52
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a5239c79_383d2ade",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 90,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "nit, this can probably be removed.  This is an existing feature of entity-manager, and doesn\u0027t really have anything to do with this design.",
      "range": {
        "startLine": 89,
        "startChar": 0,
        "endLine": 90,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3fff9514_63bcba98",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 90,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "VPDless NVMe device is against the spec, so we are eliminating the use case here.",
      "parentUuid": "a5239c79_383d2ade",
      "range": {
        "startLine": 89,
        "startChar": 0,
        "endLine": 90,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "3a10ce45_6e57b7a2",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "This is an example of something we generally shouldn\u0027t be doing, and doesn\u0027t really seem to have any requirements backing it.\n\nAs a rule, we should not be putting protocol-specific things (like mctp) on dbus, because it limits our ability to add new protocols going forward.\n\nThere is one counter example that I\u0027m aware of (ipmb) and even in that case, we would not have built it that way today, and ideally we will migrate away from it in the future.\n\n\nThis interface should be exposing high level primitives, like storage, sensors, ect.",
      "range": {
        "startLine": 95,
        "startChar": 0,
        "endLine": 101,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c6bc8e7b_7141f5c3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "\u003e This is an example of something we generally shouldn\u0027t be doing, and doesn\u0027t really seem to have any requirements backing it.\n\nJust to clarify the separation of components here - the MCTP endpoint interfaces are not provided my this new (nvmed) design, but by the general MCTP infrastructure (mctpd), which nvmed should be using to discover the results of the MCTP enumeration process.\n\nThis dbus interface is already an accepted design; the presence of the MCTP endpoints is defined at https://github.com/openbmc/phosphor-dbus-interfaces/tree/master/yaml/xyz/openbmc_project/MCTP , and the mctpd interface at https://github.com/CodeConstruct/mctp#mctpd-usage .\n\nIn this section, I believe Hao is just using this to describe what interfaces nvmed is using, rather that what it is providing.",
      "parentUuid": "3a10ce45_6e57b7a2",
      "range": {
        "startLine": 95,
        "startChar": 0,
        "endLine": 101,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6261452f_1202dee3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-06-27T17:34:04Z",
      "side": 1,
      "message": "\u003e , but by the general MCTP infrastructure (mctpd), which nvmed should be using to discover the results of the MCTP enumeration process.\n\nThis is roughly the same design that we did for IPMB that I thought we kind of agreed caused more problems than it solved, but Jeremy is certainly more of an expert here than I am, so I\u0027ll defer to his good judgement.\n\nWith that said, I wasn\u0027t aware that the mctpd-\u003edbus design had been accepted, so that changes this a bit.  In terms of this design, can we just get this section removed, and the mctpd design doc/interfaces put in the background section?",
      "parentUuid": "c6bc8e7b_7141f5c3",
      "range": {
        "startLine": 95,
        "startChar": 0,
        "endLine": 101,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "c6a4bc6b_240c7427",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 101,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "I tried to give the reader a first impression of how mctp ep is initiated from dbus. Maybe redirect to the MCTPd doc is a good idea. \n\nRemove the interface example and add mctpd link in the background",
      "parentUuid": "6261452f_1202dee3",
      "range": {
        "startLine": 95,
        "startChar": 0,
        "endLine": 101,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6cf2e932_5e29251a",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 110,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "nit, more whitespace issues.",
      "range": {
        "startLine": 110,
        "startChar": 57,
        "endLine": 110,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "94072d02_70b9b75e",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 110,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "6cf2e932_5e29251a",
      "range": {
        "startLine": 110,
        "startChar": 57,
        "endLine": 110,
        "endChar": 66
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a67009c6_7db4230e",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "I don\u0027t have strong opinions here, but I\u0027d like to see some input from Jeremy and Matt.",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0725d142_221c67dc",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "I really don\u0027t think an MCTP-specific EM config type benefits us going forward over having more specific types (NVMe-MI, PLDM, RDE, ect).\n\nCan you think of a case where say, an nvme drive and a RDE accelerator would share code or configurations here?  Even if there were sharing, I suspect they would be minimal.",
      "parentUuid": "a67009c6_7db4230e",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "736841d0_4740219a",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2022-06-27T00:37:48Z",
      "side": 1,
      "message": "Yep, I think the NVMe(-MI) level is most suitable here. I dont see a need to expose much in the way of MCTP-specific configuration, as we already have the discovery process (MCTP Control Protocol) for that, the results of which are exposed via dbus anyway.",
      "parentUuid": "0725d142_221c67dc",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "eda8ea65_4deaaace",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-06-27T17:34:04Z",
      "side": 1,
      "message": "FWIW, also, in the case where we had a device that supported more than one of the set of nvme-mi, pldm, and RDE, we would still have a way to declare it (just declare all 3 exposes records).",
      "parentUuid": "736841d0_4740219a",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a3893e65_fde757d2",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "If the device support multiple mctp type, will that be a problem we have multiple configuration for daemons (nvme, spdm etc) to initialize the same mctp ep?",
      "parentUuid": "eda8ea65_4deaaace",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e69a7c20_97848d0d",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T04:45:15Z",
      "side": 1,
      "message": "The point of exposes records is they describe the interfaces that the drive supports.  In terms of the configuration data, no, it wouldn\u0027t be a problem for a drive to have multiple, that is the point.  Just the same as if a drive supported both an NVME-mi interface AND a TMP75.",
      "parentUuid": "a3893e65_fde757d2",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2f325202_e7405e91",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-13T22:23:26Z",
      "side": 1,
      "message": "\u003e Just the same as if a drive supported both an NVME-mi interface AND a TMP75.\n\nThe NVME-mi and TMP75 works on different physical protocol. I am thinking a different scenario: a device supporting multiple MCTP types (NVMe, PLDM, etc). I am not sure if there is such a device in practise. But if so, configuration of NVME1000 and PLDM will trigger both daemons(NVMe, PLDM) to initialize the same MCTP ep on MCTPd. \n\nIt seems to be OK on a second thought. The daemon can probe the MCTPd and do a MCTPLearn() before ep initialization.",
      "parentUuid": "e69a7c20_97848d0d",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0bb8f18c_001e1424",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 118,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T23:00:35Z",
      "side": 1,
      "message": "\u003e \u003e Just the same as if a drive supported both an NVME-mi interface AND a TMP75.\n\u003e \n\u003e The NVME-mi and TMP75 works on different physical protocol. I am thinking a different scenario: a device supporting multiple MCTP types (NVMe, PLDM, etc). I am not sure if there is such a device in practise.\n\u003e But if so, configuration of NVME1000 and PLDM will trigger both daemons(NVMe, PLDM) to initialize the same MCTP ep on MCTPd.\n\u003e \n\u003e It seems to be OK on a second thought. The daemon can probe the MCTPd and do a MCTPLearn() before ep initialization.\n\n\nYep, you got it.",
      "parentUuid": "2f325202_e7405e91",
      "range": {
        "startLine": 115,
        "startChar": 0,
        "endLine": 118,
        "endChar": 43
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9906666b_200a698b",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "It\u0027s not clear why having these very-specific interfaces on dbus is helpful, given the requirements you\u0027ve laid out.  These are arguably implementation details of the nvme subsystem, and for the most part, should probably be abstracted away from dbus.  I could see possibly wanting to expose the controllers to add a \"manager\" resource within Redfish, but that probably deserves a design doc on its own.",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "31cf54c0_2eead8d7",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "MI CMD set works at the subsystem scope but the Admin CMD set needs to be attached to the controllers. LogPages are independent across controllers too. \n\nWe also want to expose the Physical Function-Virtual Function relation since they include a different set of vendor defined commands. The VU cmd is not discussed in this doc but we need to prepare for it.",
      "parentUuid": "9906666b_200a698b",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1aad18a7_1a1cf7a0",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T04:45:15Z",
      "side": 1,
      "message": "\u003e MI CMD set works at the subsystem scope but the Admin CMD set needs to be attached to the controllers. LogPages are independent across controllers too. \n\nWhy does any of this need to be exposed to dbus?  Generally we wouldn\u0027t implement a \"raw\" nvme-mi command on dbus, and instead would expose the relavant sensors, inventory items, and other things.\n\n\u003e \n\u003e We also want to expose the Physical Function-Virtual Function relation since they include a different set of vendor defined commands. The VU cmd is not discussed in this doc but we need to prepare for it.\n\nI\u0027m not sure why this changes anything.  Vendor defined commands can be put in the same daemon, same as we do with things like IPMBSensor.",
      "parentUuid": "31cf54c0_2eead8d7",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1c78dc66_1868ceaa",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-13T22:02:13Z",
      "side": 1,
      "message": "\u003e Why does any of this need to be exposed to dbus?  Generally we wouldn\u0027t implement a \"raw\" nvme-mi command on dbus, and instead would expose the relavant sensors, inventory items, and other things.\n\nUnderstand that we are trying not to expose the protocol on DBus. The problem is it is hard to map all NVMe structure/attributes/method to a DBus sensor interface. Swordfish is inventing a lot of new schemas stressing NVMe, but there is still quite behind our need (Get/Set Features, Security Send/Recv, and many other attributes). We are pushing the DMTF, but until that perfect world, we still need raw NVMe interface on DBus for these not-scoped information. \n\nI doubt whether we want to go alone the way of transplanting NVMe on DBus sensors even with a complete Swordfish define. The current mapping guide for swordfish is 300ish pages. We need a similar thing for DBus NVMe sensor. \n\nI have a feeling the BMCWeb is the place for the swordfish translation out of raw NVMe API. other generic NVMe sensors (such as the current thermal one) can build around the raw NVMe protocol daemon.",
      "parentUuid": "1aad18a7_1a1cf7a0",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c65097ce_e735805a",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T23:00:35Z",
      "side": 1,
      "message": "\u003e \u003e Why does any of this need to be exposed to dbus?  Generally we wouldn\u0027t implement a \"raw\" nvme-mi command on dbus, and instead would expose the relavant sensors, inventory items, and other things.\n\u003e \n\u003e Understand that we are trying not to expose the protocol on DBus. The problem is it is hard to map all NVMe structure/attributes/method to a DBus sensor interface.\n\nProbably going to disagree with \"hard\" here.  If we can get a list of the properties you want to expose, lets take a look to evaluate how hard this is.\n\n\u003e Swordfish is inventing a lot of new schemas stressing NVMe, but there is still quite behind our need (Get/Set Features, Security Send/Recv, and many other attributes). We are pushing the DMTF, but until that perfect world, we still need raw NVMe interface on DBus for these not-scoped information. \n\nI\u0027m failing to make the jump between \"DMTF doesn\u0027t have schemas\" and \"Therefore we need a raw dbus interface\".  Dbus can have a properly structured interface, and if someone wants to map that to non-redfish, they can.\n\n\u003e \n\u003e I doubt whether we want to go alone the way of transplanting NVMe on DBus sensors even with a complete Swordfish define.\n\nIt doesn\u0027t have to be \"complete\" but it can be incremental.\n\n\u003e The current mapping guide for swordfish is 300ish pages. We need a similar thing for DBus NVMe sensor. \n\nWhat mapping guide are you talking about?\n\n\n\n\u003e \n\u003e I have a feeling the BMCWeb is the place for the swordfish translation out of raw NVMe API.\n\n\nAs the bmcweb maintainer, definitely not.  bmcweb aspires to push business logic to other daemons to reduce its security scope, and decrease the likelihood of a bug taking down the whole OOB interface.  What you\u0027re suggesting is essentially in direct opposition.  Swordfish schemas should definitely go in bmcweb, but we should be doing NVMe-MI -\u003e Dbus -\u003e Redfish, not NVMe-MI -\u003e Redfish.\n\n\u003e other generic NVMe sensors (such as the current thermal one) can build around the raw NVMe protocol daemon.\n\nThis breaks the principal of one daemon per interface;  Also, if this is your plan, can you please include it in your design doc?",
      "parentUuid": "1c78dc66_1868ceaa",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "33e87f60_571058d6",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-15T00:38:10Z",
      "side": 1,
      "message": "\u003e \u003e \u003e Why does any of this need to be exposed to dbus?  Generally we wouldn\u0027t implement a \"raw\" nvme-mi command on dbus, and instead would expose the relavant sensors, inventory items, and other things.\n\u003e \u003e \n\u003e \u003e Understand that we are trying not to expose the protocol on DBus. The problem is it is hard to map all NVMe structure/attributes/method to a DBus sensor interface.\n\u003e \n\u003e Probably going to disagree with \"hard\" here.  If we can get a list of the properties you want to expose, lets take a look to evaluate how hard this is.\n\u003e \n\nSure. I will put a list of matrix we need for the stage 1. \n\n\u003e \u003e Swordfish is inventing a lot of new schemas stressing NVMe, but there is still quite behind our need (Get/Set Features, Security Send/Recv, and many other attributes). We are pushing the DMTF, but until that perfect world, we still need raw NVMe interface on DBus for these not-scoped information. \n\u003e \n\u003e I\u0027m failing to make the jump between \"DMTF doesn\u0027t have schemas\" and \"Therefore we need a raw dbus interface\".  Dbus can have a properly structured interface, and if someone wants to map that to non-redfish, they can.\n\u003e \n\nYes. But that means the DBus need to define some NVMe interface beyond/before Swordfish. The may fork the Swordfish/dbus definition path. To the contrast, raw NVMe API is also the golden standard. \n\n\u003e \u003e \n\u003e \u003e I doubt whether we want to go alone the way of transplanting NVMe on DBus sensors even with a complete Swordfish define.\n\u003e \n\u003e It doesn\u0027t have to be \"complete\" but it can be incremental.\n\u003e \n\u003e \u003e The current mapping guide for swordfish is 300ish pages. We need a similar thing for DBus NVMe sensor. \n\u003e \n\u003e What mapping guide are you talking about?\n\u003e \n\u003e \n\u003e \n\u003e \u003e \n\u003e \u003e I have a feeling the BMCWeb is the place for the swordfish translation out of raw NVMe API.\n\u003e \n\u003e \n\u003e As the bmcweb maintainer, definitely not.  bmcweb aspires to push business logic to other daemons to reduce its security scope, and decrease the likelihood of a bug taking down the whole OOB interface.  What you\u0027re suggesting is essentially in direct opposition.  Swordfish schemas should definitely go in bmcweb, but we should be doing NVMe-MI -\u003e Dbus -\u003e Redfish, not NVMe-MI -\u003e Redfish.\n\nFully understand the concern from BMCWeb maintenance. There is still a daemon behind BMCWeb to talk with. It is not proposed to let BMCWeb directly link to driver library. The argument is to attach a NVMe spec styled DBus interface to the daemon.\n\nI am trying to make the BMCWeb logic as simple as possible, tunneling the GET/PUT into NVMe DWORD read, write. The parsing/mapping is definitely an extra work on BMCWeb. But all other logic (initialization, association, caching, etc) should be done in the nvme daemon. \n\n\u003e \n\u003e \u003e other generic NVMe sensors (such as the current thermal one) can build around the raw NVMe protocol daemon.\n\u003e \n\u003e This breaks the principal of one daemon per interface;  Also, if this is your plan, can you please include it in your design doc?\n\nThe relationship of NVMe (thermal) sensor and NVMed will be like ipmb sensor and ipmbd. \n\nI can definitely include that in the design doc.",
      "parentUuid": "c65097ce_e735805a",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c23fb738_d45607ce",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 131,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-21T02:41:17Z",
      "side": 1,
      "message": "\u003e \n\u003e The relationship of NVMe (thermal) sensor and NVMed will be like ipmb sensor and ipmbd. \n\u003e \n\nA quick history less on how ipmb was built, and why we did that design.\n\nAt the time that was done, there was no in-kernel ipmb interface, so the ipmbd was created to store the locking between the various entities within the bmc when they tried to access in parallel and maintain the queues.\n\nIn the years after Dawid originaly built ipmbd, the ipmb driver was put into the kernel, which could maintain all that shared locking in within the driver model (which architecturally is a better place to put it).\n\nipmb was then ported to use the in-kernel model, because it was a simple patch, and made it incrementally better without breaking anyone.  Architecturally these days, it doesn\u0027t need to exist, and could easily be rolled out, with daemons like ipmbsensor directly talking to the ipmb driver.\n\nmctp is already in the kernel (we did it this way on purpose), which means comparing it to ipmb isn\u0027t really a good reason to make a dedicated daemon.",
      "parentUuid": "33e87f60_571058d6",
      "range": {
        "startLine": 129,
        "startChar": 0,
        "endLine": 131,
        "endChar": 49
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "43a5998b_34d4e909",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 143,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "I suspect this should be \"enumerating\"",
      "range": {
        "startLine": 143,
        "startChar": 58,
        "endLine": 143,
        "endChar": 69
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "045bfd1c_dd3507f8",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 143,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "43a5998b_34d4e909",
      "range": {
        "startLine": 143,
        "startChar": 58,
        "endLine": 143,
        "endChar": 69
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "fa2b2c7d_9511fbd7",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "This is a pretty generalized anti-pattern that we found when writing the other nvme daemons, as well as the generalized dbus-sensors architecture.  Keeping the polling as close to the hardware as can, and relying on dbus eventing for the rest allows use cases like EventService to function, and leads to faster responding user APIs.  It would help a lot if you could enumerate _why_ you think you need this, and we can talk through the use cases, but there have been plenty of examples where we built things as you describe above, and had to rewrite them later to go to user-space polling.",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8e4f7ded_e7d7d287",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "I listed the reasons against local polling in the alternative section. \n\ntl;dr: \nThe NVMe OOB doesn\u0027t support async event and we need high level DC service to poll in order to reduce interference.",
      "parentUuid": "fa2b2c7d_9511fbd7",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f82b9604_3e94b0ee",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T04:45:15Z",
      "side": 1,
      "message": "\u003e The NVMe OOB doesn\u0027t support async event \n\nThe existing daemon does support async eventing from the kernel. (and took a lot of architecture work to do it).",
      "parentUuid": "8e4f7ded_e7d7d287",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9d909e82_c66b8aa3",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-13T21:13:00Z",
      "side": 1,
      "message": "I may be wrong but I am not sure whether we are talking about the same. Async Event Request is prohibited on OOB mechanism(Figure 114. NVMe MI spec rev 1.2). \n\nWe prefer the upper layer SW (outside BMC) to perform active request model instead of BMC local polling, It benefit the coordination between inband and oob nvme transaction.",
      "parentUuid": "f82b9604_3e94b0ee",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "442b80cf_e29ffbe1",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T23:00:35Z",
      "side": 1,
      "message": "Yes, we were talking about different things.  Maybe make it more clear that runtime status is coming from the host?  I\u0027m not sure I fully understand in this case.",
      "parentUuid": "9d909e82_c66b8aa3",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8b13b4c6_e37533a5",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-15T00:38:10Z",
      "side": 1,
      "message": "I mean the NVMe OOB controller has no mechanism to push the status change to host software (ie. the bmc and upper layer DC SW). \n\nSo it is about who pulls the info, either BMC local polling or DC SW polling. We do the latter one because: \n1. The DC SW will coordinate with other DC service, such as FW updater, user management, clean up service and repair service about when to operate NVMe OOB. These services definitely outside the scope of NVMed and maybe outside BMC. \n2. The OOB may/will interfere with Inband transaction. We haven\u0027t evaluated the impact but the concern is in the NVMe Spec. We are certainly trying to avoid reading a large status table out of NVMe-MI while a heavy NVMe I/O task.",
      "parentUuid": "442b80cf_e29ffbe1",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8c7469b8_a5d01128",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 145,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-21T02:41:17Z",
      "side": 1,
      "message": "\u003e I mean the NVMe OOB controller has no mechanism to push the status change to host software (ie. the bmc and upper layer DC SW). \n\n\nI thought there were status bits that we could poll rapidly in nvme, but sure, I understand your meaning now.\n\n\u003e \n\u003e So it is about who pulls the info, either BMC local polling or DC SW polling. We do the latter one because: \n\u003e 1. The DC SW will coordinate with other DC service, such as FW updater, user management, clean up service and repair service about when to operate NVMe OOB. These services definitely outside the scope of NVMed and maybe outside BMC. \n\nTotally fine, and nothing I was concerned about would prevent that\n\n\u003e 2. The OOB may/will interfere with Inband transaction.\n\u003e We haven\u0027t evaluated the impact but the concern is in the NVMe Spec. We are certainly trying to avoid reading a large status table out of NVMe-MI while a heavy NVMe I/O task.\n\n\nAccording to the NVMe spec it shouldn\u0027t I thought?  They are separate channels.  Certainly not the interfaces that are expected to be polled, like thermal sensors and log pages.  Is there something in the spec you\u0027re looking at that makes you think otherwise?  Can you give me a section number?\n\n\u003e We are certainly trying to avoid reading a large status table out of NVMe-MI while a heavy NVMe I/O task.\n\nThe log pages have a polling mechanism, and the drives I\u0027ve worked with from major vendors in the past have accounted for this kind of thing in their software design, I\u0027d be really surprised if there were drives that didn\u0027t.",
      "parentUuid": "8b13b4c6_e37533a5",
      "range": {
        "startLine": 143,
        "startChar": 71,
        "endLine": 145,
        "endChar": 29
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d2538a19_66e03291",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "Repeating again, all of the above is nvme-specific, and makes no attempt to reuse existing interfaces.  As-written, to accomplish your goals, this is going to require both IPMI and bmcweb to contain nvme-specific code about how to identify drives, read log pages, and do whatever else needs done.  I suspect this needs abstracted using the existing interfaces.  I suspect there\u0027s one or two interfaces that need added for the drive specific inventory things that we don\u0027t yet support, but as written, I don\u0027t think the above interfaces get us the level of abstraction on dbus that we generally would like to encourage.",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "5b570753_16543c08",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-12T22:37:16Z",
      "side": 1,
      "message": "Agree on the first half. We need to reuse/expose the general Drive/Storge (maybe StorageController and Volume) interface with generic properties/methods. \n\nBut there are still NVMe specific which is hard/impossible to be defined on DBus level (e.g NVMe Feature set and controller Identify info). so we still need a NVMe API on DBus.",
      "parentUuid": "d2538a19_66e03291",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "61855eb0_f305dfa4",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T04:45:15Z",
      "side": 1,
      "message": "So make an xyz.openbmc_project.NVMeFeatureSet dbus interface and attach it to the drive?",
      "parentUuid": "5b570753_16543c08",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a9848637_825d4068",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-13T21:13:00Z",
      "side": 1,
      "message": "\u003e So make an xyz.openbmc_project.NVMeFeatureSet dbus interface and attach it to the drive?\n\nThere are more information polling from other NVMe CMD(Read NVMe-MI Data Structure, Identify, etc). Some of the information we need is: \n\n* Health status for the NVMe subsystem including warning(SW), temperature(CTMP), drive life(PDLU), contoller status(CCS).\n* Configuration for NVMe controller such as SMBus/I2C frequency and MCTP Transmission Unit Size.\n* Features for controllers involving Arbitration, Power Management, Controller Metadata, etc.\n* Identify information for controller and namespace, such as FW info, SN/MN, IEEE OUI Identifier(IEEE), Maximum Data Transfer Size(MDTS), RTD3 Resume Latency(RTD3R), RTD3 Entry Latency(RTD3E), etc\n\nSome of these information is storage generic but more is NVMe specific. \n\nThe point is, we try to avoid exhausting all definitions in NVMe spec and redo it on dbus. Swordfish does the mapping well (or trying to approach that). The new daemon should work as NVMe protocol daemon(like ipmbd?). The consumer services, such as BMCWeb, should do the protocol translation. BMCWeb has the schemas and the translation layer in our management node (outside BMC) has the protobuf for grpc/streamz.",
      "parentUuid": "61855eb0_f305dfa4",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "371b80ee_86ca31b4",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-13T23:00:35Z",
      "side": 1,
      "message": "\u003e \u003e So make an xyz.openbmc_project.NVMeFeatureSet dbus interface and attach it to the drive?\n\u003e \n\u003e There are more information polling from other NVMe CMD(Read NVMe-MI Data Structure, Identify, etc). Some of the information we need is: \n\u003e \n\u003e * Health status for the NVMe subsystem including warning(SW), \n\nStatus interfaces have been around for a while.\n\n\u003etemperature(CTMP),\n\nSo.... this is duplicating temperature code?  This is sounding more and more like it needs to go in the existing daemon.\n\n\u003e drive life(PDLU), contoller status(CCS).\n\u003e * Configuration for NVMe controller such as SMBus/I2C frequency and MCTP Transmission Unit Size.\n\nThat kind of data shouldn\u0027t be on dbus (probably).  Those are arguably internal system details of the communication channel that should be abstracted away from dbus.\n\n\u003e * Features for controllers involving Arbitration, Power Management, Controller Metadata, etc.\n\nWhy is this information required outside the daemon itself that\u0027s talking to the controller?  Wouldn\u0027t this just get exposed as normal reset interfaces?\n\n\u003e * Identify information for controller and namespace, such as FW info, SN/MN, IEEE OUI Identifier(IEEE), Maximum Data Transfer Size(MDTS), RTD3 Resume Latency(RTD3R), RTD3 Entry Latency(RTD3E), etc\n\nMost of these likely map into the Asset interfaces.  For the ones that don\u0027t, we can add new interfaces.\n\n\u003e \n\u003e Some of these information is storage generic but more is NVMe specific. \n\n\u003e \n\u003e The point is, we try to avoid exhausting all definitions in NVMe spec and redo it on dbus.\n\nUnfortunately, that just means that we get to duplicate all the definitions in bmcweb/redfish/ipmi.  As a system-level design, we try to avoid that.\n\n\u003e Swordfish does the mapping well (or trying to approach that). The new daemon should work as NVMe protocol daemon(like ipmbd?). The consumer services, such as BMCWeb, should do the protocol translation. BMCWeb has the schemas and the translation layer \n\nbmcweb translates dbus-\u003eredfish, and we try to keep as much business logic out ofit that we can to keep a small footprint, and to spread out the security consequences and failure modes.  Redfish very intentionally doesn\u0027t allow \"transports\" like this.\n\n\u003ein our management node (outside BMC) has the protobuf for grpc/streamz.",
      "parentUuid": "a9848637_825d4068",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c8f5db19_c4d05e25",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000857
      },
      "writtenOn": "2022-07-15T00:38:10Z",
      "side": 1,
      "message": "\u003e \u003e \u003e So make an xyz.openbmc_project.NVMeFeatureSet dbus interface and attach it to the drive?\n\u003e \u003e \n\u003e \u003e There are more information polling from other NVMe CMD(Read NVMe-MI Data Structure, Identify, etc). Some of the information we need is: \n\u003e \u003e \n\u003e \u003e * Health status for the NVMe subsystem including warning(SW), \n\u003e \n\u003e Status interfaces have been around for a while.\n\nYes, the every storage protocol has a SMART-like info. So this part can be generic. \n\n\u003e \n\u003e \u003etemperature(CTMP),\n\u003e \n\u003e So.... this is duplicating temperature code?  This is sounding more and more like it needs to go in the existing daemon.\n\u003e \n\nYes. There are two thermal reporting path. The pid service internally will work on the NVMe thermal sensor for NVMe subsystem. The DC need extra thermal info for each NVMe controller through NVMe SMART Health Log Page, which will report by NVMed-\u003eredfish-\u003eDC SW\n\n\u003e \u003e drive life(PDLU), contoller status(CCS).\n\u003e \u003e * Configuration for NVMe controller such as SMBus/I2C frequency and MCTP Transmission Unit Size.\n\u003e \n\u003e That kind of data shouldn\u0027t be on dbus (probably).  Those are arguably internal system details of the communication channel that should be abstracted away from dbus.\n\u003e \n\u003e \u003e * Features for controllers involving Arbitration, Power Management, Controller Metadata, etc.\n\u003e \n\u003e Why is this information required outside the daemon itself that\u0027s talking to the controller?  Wouldn\u0027t this just get exposed as normal reset interfaces?\n\u003e \n\nThe reason is currently out the the control of BMC. The DC SW will ask the BMC to report the matrix and extract the information they like. I will talk to the DC SW team for the specific attributes, but the first answer I got from them was \"that will be a long list\". \n\n\n\u003e \u003e * Identify information for controller and namespace, such as FW info, SN/MN, IEEE OUI Identifier(IEEE), Maximum Data Transfer Size(MDTS), RTD3 Resume Latency(RTD3R), RTD3 Entry Latency(RTD3E), etc\n\u003e \n\u003e Most of these likely map into the Asset interfaces.  For the ones that don\u0027t, we can add new interfaces.\n\u003e \n\nThey could be but they need to be part of StorageController. \nAnyway, I will make a list of the specific matrix so we can start from there. \n\n\u003e \u003e \n\u003e \u003e Some of these information is storage generic but more is NVMe specific. \n\u003e \n\u003e \u003e \n\u003e \u003e The point is, we try to avoid exhausting all definitions in NVMe spec and redo it on dbus.\n\u003e \n\u003e Unfortunately, that just means that we get to duplicate all the definitions in bmcweb/redfish/ipmi.  As a system-level design, we try to avoid that.\n\nSorry, I didn\u0027t get that. Why it will cause a duplication in bmcweb/redfish/ipmi? Actually the reason we create a NVMe raw interface on DBus is to avoid duplicating the definition. \n\nThe reasoning here is that the NVMe information can be parsed at different levels. The generic storage info can be parse at daemon level for ipmi/redfish part of BMCWeb. The Swordfish of BMCWeb can parse the NVMe specific information from the DBus NVMe raw interface. The DCSW can parse rest of the unscoped NVMe-MI and NVMe-MI VUC. \n\n\u003e \n\u003e \u003e Swordfish does the mapping well (or trying to approach that). The new daemon should work as NVMe protocol daemon(like ipmbd?). The consumer services, such as BMCWeb, should do the protocol translation. BMCWeb has the schemas and the translation layer \n\u003e \n\u003e bmcweb translates dbus-\u003eredfish, and we try to keep as much business logic out ofit that we can to keep a small footprint, and to spread out the security consequences and failure modes.  Redfish very intentionally doesn\u0027t allow \"transports\" like this.\n\u003e \n\nCopied from the other commit: \n\nFully understand the concern from BMCWeb maintenance. There is still a daemon behind BMCWeb to talk with. It is not proposed to let BMCWeb directly link to driver library. The argument is to attach a NVMe spec styled DBus interface to the daemon.\n\nI am trying to make the BMCWeb logic as simple as possible, tunneling the GET/PUT into NVMe DWORD read, write. The parsing/mapping is definitely an extra work on BMCWeb. But all other logic (initialization, association, caching, etc) should be done in the nvme daemon. \n\n\u003e \u003ein our management node (outside BMC) has the protobuf for grpc/streamz.",
      "parentUuid": "371b80ee_86ca31b4",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "78256e58_2c528d0c",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 179,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-07-21T02:41:17Z",
      "side": 1,
      "message": "\u003e \u003e \u003e \u003e So make an xyz.openbmc_project.NVMeFeatureSet dbus interface and attach it to the drive?\n\u003e \u003e \u003e \n\u003e \u003e \u003e There are more information polling from other NVMe CMD(Read NVMe-MI Data Structure, Identify, etc). Some of the information we need is: \n\u003e \u003e \u003e \n\u003e \u003e \u003e * Health status for the NVMe subsystem including warning(SW), \n\u003e \u003e \n\u003e \u003e Status interfaces have been around for a while.\n\u003e \n\u003e Yes, the every storage protocol has a SMART-like info. So this part can be generic. \n\u003e \n\u003e \u003e \n\u003e \u003e \u003etemperature(CTMP),\n\u003e \u003e \n\u003e \u003e So.... this is duplicating temperature code?  This is sounding more and more like it needs to go in the existing daemon.\n\u003e \u003e \n\u003e \n\u003e Yes. There are two thermal reporting path.\n\u003e The pid service internally will work on the NVMe thermal sensor for NVMe subsystem.\n\u003e The DC need extra thermal info for each NVMe controller through NVMe SMART Health Log Page, which will report by NVMed-\u003eredfish-\u003eDC SW\n\nI don\u0027t think this works in the design.  This will\n\n1. Duplicate state on dbus\n2. Duplicate code in the backend.\n\nNeither of which I\u0027m interested in maintaining.\n\n\u003e \n\u003e \u003e \u003e drive life(PDLU), contoller status(CCS).\n\u003e \u003e \u003e * Configuration for NVMe controller such as SMBus/I2C frequency and MCTP Transmission Unit Size.\n\u003e \u003e \n\u003e \u003e That kind of data shouldn\u0027t be on dbus (probably).  Those are arguably internal system details of the communication channel that should be abstracted away from dbus.\n\u003e \u003e \n\u003e \u003e \u003e * Features for controllers involving Arbitration, Power Management, Controller Metadata, etc.\n\u003e \u003e \n\u003e \u003e Why is this information required outside the daemon itself that\u0027s talking to the controller?  Wouldn\u0027t this just get exposed as normal reset interfaces?\n\u003e \u003e \n\u003e \n\u003e The reason is currently out the the control of BMC.\n\nIf it\u0027s out of control of the bmc, then it\u0027s going through an OOB interface, (based on your doc Redfish).  Redfish requires well structured data.\n\n\u003e The DC SW will ask the BMC to report the matrix and extract the information they like. I will talk to the DC SW team for the specific attributes, but the first answer I got from them was \"that will be a long list\".\n\nGreat.  Lets get that list and go from there.\n\n\u003e \n\u003e \n\u003e \u003e \u003e * Identify information for controller and namespace, such as FW info, SN/MN, IEEE OUI Identifier(IEEE), Maximum Data Transfer Size(MDTS), RTD3 Resume Latency(RTD3R), RTD3 Entry Latency(RTD3E), etc\n\u003e \u003e \n\u003e \u003e Most of these likely map into the Asset interfaces.  For the ones that don\u0027t, we can add new interfaces.\n\u003e \u003e \n\u003e \n\u003e They could be but they need to be part of StorageController.\n\nThat\u0027s not a problem.  Asset is used in a number of places, StorageController will just be one more\n\n\u003e Anyway, I will make a list of the specific matrix so we can start from there. \n\u003e \n\u003e \u003e \u003e \n\u003e \u003e \u003e Some of these information is storage generic but more is NVMe specific. \n\u003e \u003e \n\u003e \u003e \u003e \n\u003e \u003e \u003e The point is, we try to avoid exhausting all definitions in NVMe spec and redo it on dbus.\n\u003e \u003e \n\u003e \u003e Unfortunately, that just means that we get to duplicate all the definitions in bmcweb/redfish/ipmi.  As a system-level design, we try to avoid that.\n\u003e \n\u003e Sorry, I didn\u0027t get that. Why it will cause a duplication in bmcweb/redfish/ipmi? Actually the reason we create a NVMe raw interface on DBus is to avoid duplicating the definition. \n\nWhen bmcweb wants to get a temperature reading, or a log page, it will need to \"speak\" nvme protocol on dbus.  This requires data structures that know how to package an nvme request, and interpret an nvme response.\n\nIf I now want that same data on IPMI, which maybe you don\u0027t want, but I\u0027m sure others do, now IPMI has to copy those same data structures over.  We avoid this by putting well formed and reasonable abstractions on dbus.\n\n\u003e \n\u003e The reasoning here is that the NVMe information can be parsed at different levels.\n\nTo be clear, we have a mechanism for parsing log pages and putting them on dbus.  That\u0027s reasonable and there\u0027s prior interfaces for that.\n\n\u003e The generic storage info can be parse at daemon level for ipmi/redfish part of BMCWeb. The Swordfish of BMCWeb can parse the NVMe specific information from the DBus NVMe raw interface.\n\nbmcweb knows how to parse well structured dbus interfaces, like sensors and logs.  Once those are mapped, we can discuss how to handle other stuff, including if you want some kind of \"raw nvme socket\" like we have for serial or something, but it\u0027s not going to fit in well formed redfish payloads.\n\n\u003e The DCSW can parse rest of the unscoped NVMe-MI and NVMe-MI VUC. \n\nPlease talk about how this would work at an interfacing level.\n\n\u003e \n\u003e \u003e \n\u003e \u003e \u003e Swordfish does the mapping well (or trying to approach that). The new daemon should work as NVMe protocol daemon(like ipmbd?). The consumer services, such as BMCWeb, should do the protocol translation. BMCWeb has the schemas and the translation layer \n\u003e \u003e \n\u003e \u003e bmcweb translates dbus-\u003eredfish, and we try to keep as much business logic out ofit that we can to keep a small footprint, and to spread out the security consequences and failure modes.  Redfish very intentionally doesn\u0027t allow \"transports\" like this.\n\u003e \u003e \n\u003e \n\u003e Copied from the other commit: \n\u003e \n\u003e Fully understand the concern from BMCWeb maintenance. There is still a daemon behind BMCWeb to talk with. It is not proposed to let BMCWeb directly link to driver library.\n\u003eThe argument is to attach a NVMe spec styled DBus interface to the daemon.\n\nThe linking is one concern, needing to parse, construct, and interpret NVMe messages on dbus is a much bigger concern.\n\n\u003e \n\u003e I am trying to make the BMCWeb logic as simple as possible, tunneling the GET/PUT into NVMe DWORD read, write.\n\nSo, you\u0027re not using redfish then?  Redfish definitely doesn\u0027t give a \"DWORD Write\" interface, and has very explicitly rejected it at a standards level many times.  I\u0027m in the DMTF meetings every week if you\u0027d like to open a thread and start that discussion again.\n\n\u003e The parsing/mapping is definitely an extra work on BMCWeb.\n\nand IPMI.  And PLDM.   More importantly, we have patterns in the project that avoid this extra work and we should use them.\n\n\u003e But all other logic (initialization, association, caching, etc) should be done in the nvme daemon.\n\u003e \n\u003e \u003e \u003ein our management node (outside BMC) has the protobuf for grpc/streamz.",
      "parentUuid": "c8f5db19_c4d05e25",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9b6fcb95_37f2a30b",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 187,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "This isn\u0027t in your requirements.",
      "range": {
        "startLine": 186,
        "startChar": 53,
        "endLine": 187,
        "endChar": 44
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5dc01f5b_9349d36a",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 191,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "Can you rephrase this, I\u0027m not quite understanding this.",
      "range": {
        "startLine": 190,
        "startChar": 35,
        "endLine": 191,
        "endChar": 70
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2a4055ff_396573af",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 196,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "Other impacts:\n1. Physical layer constructs are now present in dbus, which very likely has security impacts.\n2. nvme-specific is now spread across the system\n3. A third nvme-specific daemon is created.",
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "873f8e44_0c2a3e75",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 200,
      "author": {
        "id": 1000009
      },
      "writtenOn": "2022-05-20T03:04:26Z",
      "side": 1,
      "message": "Okay, can you please add some more words justifying why they\u0027re complementary but must be separate? Am I just not across enough of the design points of phosphor-nvme? If not, can you please make sure that people like me reading this document can be convinced of your position just by reading the document (and any references)?",
      "range": {
        "startLine": 199,
        "startChar": 0,
        "endLine": 200,
        "endChar": 35
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "03ba57bd_69a33fff",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 200,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "\u003e Okay, can you please add some more words justifying why they\u0027re complementary but must be separate?\n\u003e Am I just not across enough of the design points of phosphor-nvme? If not, can you please make sure that people like me reading this document can be convinced of your position just by reading the document (and any references)?\n\n+1, I\u0027d really like to understand this point as well.  It\u0027s not clear from the above.",
      "parentUuid": "873f8e44_0c2a3e75",
      "range": {
        "startLine": 199,
        "startChar": 0,
        "endLine": 200,
        "endChar": 35
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "718664c2_27be5864",
        "filename": "designs/nvmed.md",
        "patchSetId": 1
      },
      "lineNbr": 205,
      "author": {
        "id": 1000153
      },
      "writtenOn": "2022-05-23T15:53:00Z",
      "side": 1,
      "message": "Load testing?  Security testing?  Functional testing?",
      "range": {
        "startLine": 204,
        "startChar": 0,
        "endLine": 205,
        "endChar": 54
      },
      "revId": "56f76956a03e879a682b57dd0bbd56b84e1a15be",
      "serverId": "adbd0a64-1f21-4d83-b585-671fe73cb6e4"
    }
  ]
}